<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta http-equiv="X-UA-Compatible" content="ie=edge" />
      <style>
.mouth {
  d: path("m238,137 q 20,25 40,0 q -20,5 -40,0 "); 
  animation: cover 1.5s ease-in 900;
}


@keyframes cover {
    
  0% {
     d: path("m238,137 q 20,25 40,0 q -20,5 -40,0 ");
  }
  20% {
     d: path("m238,137 q 20,35 40,0 q -20,-5 -40,0 ");
  }

  80% {
    d: path("m238,137 q 20,15 40,0 q -20,15 -40,0 ");
  }
}
 
</style>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/jsrsasign/8.0.4/jsrsasign-all-min.js"></script>
	<script src="https://apis.google.com/js/api.js"></script>
    <title>speech recognition and query</title>
  </head>
  <body>
    <header>
      <h1>Browser speech recognition and talking</h1>
    </header>
    <main>
<div id="face">
 <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500">
 <g stroke="black" stroke-width="2" fill="none" >
 
    </g>
    <g>
  <title>Layer 1</title>
  <ellipse id="svg_2" cy="82" cx="159" stroke-width="5" stroke="#000000" fill="#FF0000"/>
  <ellipse ry="92" rx="87" id="svg_3" cy="221" cx="261" stroke="#000000" fill="#cccccc"/>
  <ellipse id="svg_4" cy="583" cx="146" stroke="#000000" fill="#cccccc"/>
  <rect stroke-width="0" id="svg_5" height="113" width="189" y="67" x="166" stroke="#000000" fill="#cccccc"/>
  <rect id="svg_6" height="0" width="1" y="-58" x="-181" stroke-width="2" stroke="#000000" fill="#cccccc"/>
  <circle id="svg_7" r="17.80449" cy="107" cx="197" stroke-width="2" stroke="#000000" fill="#000000"/>
  <circle id="svg_9" r="18.43909" cy="104" cx="320" stroke-width="2" stroke="#000000" fill="#000000"/>
  <circle id="svg_10" r="1" cy="-60" cx="198" stroke-width="2" stroke="#000000" fill="#000000"/>
  <rect id="svg_11" height="18" width="51" y="206" x="123" stroke-width="2" stroke="#000000" fill="#000000"/>
  <rect id="svg_12" height="23" width="84" y="206" x="346" stroke-width="2" stroke="#000000" fill="#000000"/>
  <rect transform="rotate(28.967660903930664 108.50000000000006,200) " id="svg_13" height="18" width="33" y="191" x="92" stroke-width="2" stroke="#000000" fill="#cccccc"/>
  <rect id="svg_15" transform="rotate(-39.805572509765625 112.99999999999999,234.00000000000003) " height="18" width="33" y="225" x="96.5" stroke-width="2" stroke="#000000" fill="#cccccc"/>
  <rect id="svg_16" transform="rotate(28.967660903930664 437,234.00000000000003) " height="18" width="33" y="225" x="420.5" stroke-width="2" stroke="#000000" fill="#cccccc"/>
  <rect transform="rotate(-3.0127875804901123 440,205.9999999999996) " id="svg_17" height="18" width="33" y="197" x="423.5" stroke-width="2" stroke="#000000" fill="#cccccc"/>
  <circle stroke="#000000" id="svg_18" r="56.25971" cy="258" cx="262" stroke-linecap="null" stroke-linejoin="null" stroke-dasharray="null" stroke-width="0" fill="#ffffff"/>
  <path id="mouthPath" d="m238,137 q 20,25 40,0 q -20,5-40,0" ;="" fill="red"></path>	 
 </g>
  </g>  
 </svg>
</div>

 <div>
	 
      <button id="button">Start listening</button>
      <div id="result"></div>
      <p id="message" hidden aria-hidden="true">
        Your browser doesn't support Speech Recognition. Sorry.
      </p>
	  <p id="inputText">    </p>
    </main>

    <script>
// This works well!
// process:  start-->recognition.start()
 //recognition.start() = event=> {... sendRecogitionText(...) }
//  recognition.addEventListener("result", onResult);
//  OnResult---> (sendRecogitionText(order)--->queryDialog----> sendText--->speak())
//
//===================== get token:
	let private_key_id = "c4057ead1877cc6d2bc185434e1c0fa328586a9f";
	let private_key = "-----BEGIN PRIVATE KEY-----\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQDAJOC5lfbLa7Xu\nKDvSEbTNAraFzWHRdges35XWPXwwxK6zKh+27eMpdh97fXLHbvpJXT5ijHAWSOPl\n4G/bghHlwPtvVVArGIPRTXpdC+5jsFwd/82D2brHAFVCEYlczI9/O7YXtDkmMcTA\nIcnZk+9yjcmGkyIYKxHpRcSOVoasecydj8Z4FxbmZMqvUW25UPhN6YV2kDAnPe4O\npI9ITbGP8lbTdcZalaSpGVCmAqldDtKtA31CpWvZYefUykb4IyFvykrnKjofPnTk\n17IuEwaBJOebMzQShLgZo+I+SHbDIZS9F5vVWNeYXjxOvrGtae/6ob3p3+6mqfsn\nr8ncE3QnAgMBAAECggEAUg2tljNvgDFQMRMncK81CbOV5I/vrPZ9kpqsrx/0sBLe\nFA7yGjl+n88c5KyO3pIK+leCc61LlIiuIhX1TsU+BWKjPUkugijm6fPvRFr7A8Fl\npT3/MsZBWAhRYHSCt4l34jrKqVbCbQgl4HLNc0gUolPgcsLUdYO3E2pC5RIYUwC2\nFjA+LyXnTt0TDPPxKsmem7Yo1+k1pQ+KVjWC63BgYyU+awY712C5vE30MEGjkRRH\n8buY4KjeVEME3Gc7U2ZhPRw3Q+qwCEXk7Bw6IBcFpVJiHnanwKEnqDYlwoBrHIUE\n16aS0GmseGN0sRhHqHHx9UsY4PsSNKydl3qKUnCW4QKBgQD29HWCUv5+Rq+uTLoU\nKnnxOW9mMTAf+Ae4Ix0Ivj7ABqDdKv+s6jdBr9jN08hoV96RbEdjSsK/LgSMWAvU\neY/c76F5KnUNPjNTb42AixieZp/drJV960Mfb75QF0qTx9R4Sq/bzdcNb90e7Mhc\nL5/3OZGBRnwVQDMJihMI8tomFwKBgQDHLn3m9bRB5vzCzFKZXuI2AGkRHqoKN9FH\nIMatDI6xjDGert4TYxmnfDRsVfAzXcCM3NpDVEOt/EIryljcqxHI08mKJDwtibwr\nzbH5iy9oSuOpW746PntI1WJGDTas54+Bsv+RynRq7VnIEAXSf4ooZjRpfrOS3DCC\nOKw9zkb8cQKBgCM4nGXY9mkGJdpDISrtH/UyjtvRWh7FHjy57VMo5wQ2Lc09a7e6\ns+TPcqlwch3Honu1KL1VlU70/jsR96X3LHiDSNFiJ82auHFG8LSLohg9ZMGV8sBB\nxJIAOvFrUPygvIHnjQtZYTmOYcgeVzkmpbb54/G/HsVdIapTogFduZVDAoGAY5Iw\nSlcmB5PrY4409IrkVs6Zt382zejda8tMxpFye/tzirosNlDiMEH9CvNjIqqJaWG2\nktX7B20Fv1JmXdn/CAAnzjETDEjAkr0/bVpRFrfpW26LiB5YDy7s88wDOyh/Kv1K\nmZ012YZRzZuE7Zfofw2owdwe9Rmx/kdKg5MtFWECgYABqDKZhJck6gxPvT9+Omhu\nAjm7oKKtpOoiwHpi3Cl4LVd4lkGUjchRmwS2smdwRhUyYXl0gLuashKvVz4+hN58\n1adVx653Z20fMKC8qt1cNCNwid3RPUuMG+DPwIFAgLSL6G1b+8dxU8lxNvtPq6KT\nJV8sglFcxV/xL2pzSkPRCA==\n-----END PRIVATE KEY-----\n";
    let client_email = "mytest@joke-kdxh.iam.gserviceaccount.com";
	const header = {
        alg: 'RS256',
        typ: 'JWT',
        kid: private_key_id  ////???
      }

      // Payload
      const payload = {
        iss: client_email,
        sub: client_email,
        iat: KJUR.jws.IntDate.get('now'),
        exp: KJUR.jws.IntDate.get('now + 1hour'),
        aud: 'https://dialogflow.googleapis.com/google.cloud.dialogflow.v2.Sessions'	
      }
      
      const stringHeader = JSON.stringify(header);
      const stringPayload = JSON.stringify(payload);
      var token = KJUR.jws.JWS.sign('RS256', stringHeader, stringPayload, private_key);
	
	console.log("token: " + token);
//====== Got token for Dialogflow service  
var recognition;
var timer1;
var timeron = false;
var timeStamp1;
var timeStamp2;
var base_url = 'https://dialogflow.googleapis.com/v2/projects/megaagent-9goq/agent/sessions/';
var sessionId = Math.floor(Math.random() * 10000000);
var languageCode = 'en-US';
 async function queryDialog(input_data){
 console.log("input_data2:" + input_data )
  const response = await fetch(
  base_url + sessionId + ":detectIntent", {

		    method: 'POST',	
			
			headers: {
				"Accept": "application/json, text/plain, */*",
                "Content-Type": "application/json",

                "Authorization": "Bearer " + token,

            },

			
            body: JSON.stringify(input_data)

        });
		
		
	 const data = await response.json();
	// console.log(data);
	 console.log("result: ", data.queryResult.fulfillmentText);
	 speak(data.queryResult.fulfillmentText);
};	
	// 
let inputText = document.querySelector("#inputText");
var button = document.getElementById("button");
var listening= false;
var speaking= false;
    button.addEventListener("click", event => {
      listening ? stopListening() : startListening();
	  console.log("button1:" + button);
 //           listening = !listening;
          });

const result = document.getElementById("result");
const main = document.getElementsByTagName("main")[0];  //-------
const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
 recognition = new SpeechRecognition();
// speak out!
speak("You are welcome!");		  		  

recognition.onend = function() {
		console.log('Speech recognition service end');
		let d2 = new Date();
		let tempStamp1 = d2.getTime();
		let difference1 = tempStamp1 - timeStamp1;
		console.log("recognition difference:" + difference1);
		listening = false;
		button.textContent = "Start listening";
		timeron = true;
		timer1 = setTimeout(()=>{	
		startListening();
		timeron = false;
		}, 1500);// ???????
		}
			
const onResult = event => {
        inputText.innerHTML = "";
        for (const res of event.results) {
             const text = document.createTextNode(res[0].transcript);
			 recogitionText =res[0].transcript;
             if (res.isFinal) {
                inputText.innerHTML = recogitionText;
              }

            }
		let d1 = new Date();
         timeStamp1 = d1.getTime(); // get timeStamp1 
			if(recogitionText){
		sendRecogitionText(recogitionText);
		}  
          };
		  
recognition.continuous = false;
recognition.interimResults = false; //True is the same result, why?
recognition.addEventListener("result", onResult);
		
if (typeof SpeechRecognition == "undefined") {
    button.remove();
    inputText.innerHTML = "The browser does not support speech recognition.";
        }   		

	  //////

	//go to Dialogflow:
	//
		 
	const sendRecogitionText = (order) => {
	console.log("function get order:" + order);
	//go to Dialogflow:
	//
		 
	let query =order;
	console.log("query.length:" + query.length);
	if((query.length > 0) &&(speaking == false )){
	let query_data = {"queryInput": {"text": {"text":query,"languageCode":"en-US"}}};	
	queryDialog(query_data);}
	}
	
const speak = message => {
 if(listening){
 recognition.stop;
 listening = false;
}
  const msg = new SpeechSynthesisUtterance( )
  msg.text = message;
  msg.lang = 'en-US';
  //var voices = window.speechSynthesis.getVoices();
 // msg.voice = voices[4];
  msg.pitch = 1;
  console.log("msg will be spoken.");
  console.log("message:" + message);
  window.speechSynthesis.speak(msg);
  
  msg.onstart = function(event) {
    if(listening) {
	listening = false;
	recognition.stop};
	if(timeron){
	clearTimeout(timer1);
	timeron = false;
	};
    speaking = true; 
    changeFace(); //moving lips
	let d3 = new Date();
    timeStamp2 = d3.getTime();	
    console.log('We have started uttering this speech: ' + event.utterance.text);
  }
  msg.onend = function(event) {
       mouthPath.classList.remove(...mouthPath.classList);
  	let d4 = new Date();
    tempStamp2 = d4.getTime();	
    let difference2 = tempStamp2 - timeStamp2 ;
	console.log("msg duration :" + difference2);
	console.log("speaking :" + speaking);
	console.log("listening :" + listening);
  speaking = false;
  console.log('msg has finished being spoken after ' + event.elapsedTime + ' milliseconds.');
  //if(listening == false){
  startListening(); // at end of speak, start Listening. 
  }
  //}
  
  msg.onerror = function(event){
  console.log("msg error: ")
  }
}
 
function startListening() {
  console.log("button2:" + button);
  console.log("speaking listening:" + speaking + listening );
  if((speaking == false) && (listening == false)){
  listening= true;
  // console.log("OK!OK");
  setTimeout(recognition.start(),20);
  button.textContent = "Stop listening";
  console.log("button.textContent:" + button.textContent);
   }  
  }
  
function stopListening() {
		console.log("button3:" + button);
		listening = false;
        recognition.stop();
        button.textContent = "Start listening";
         };
function changeFace() {
mouthPath.classList.remove(...mouthPath.classList);
mouthPath.classList.add("mouth");
}
  
  
 
    </script>
  </body>
</html>
